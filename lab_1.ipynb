{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Bring your own container to SageMaker Studio\n",
    "\n",
    "In this notebook, you create your own Docker image and build a processing container. You use a **ScriptProcessor** class from the Amazon SageMaker Python SDK to run a scikit-learn preprocessing script within the container. Then, you validate the data processing results that are saved in Amazon Simple Storage Service (Amazon S3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.1: Environment setup\n",
    "\n",
    "Install the required libraries and dependencies.\n",
    "\n",
    "You set up an Amazon S3 bucket to store the outputs from the processing job and also get the execution role to run the SageMaker processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# add csv visualization and image-build packages\n",
    "%pip install sagemaker-studio-image-build \n",
    "%pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install-dependencies\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "sagemaker_logger = logging.getLogger(\"sagemaker\")\n",
    "sagemaker_logger.setLevel(logging.INFO)\n",
    "sagemaker_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "#Execution role to run the SageMaker Processing job\n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"SageMaker Execution Role: \", role)\n",
    "\n",
    "#S3 bucket to read the SKLearn processing script and writing processing job outputs\n",
    "s3 = boto3.resource('s3')\n",
    "for buckets in s3.buckets.all():\n",
    "    if 'databucket' in buckets.name:\n",
    "        bucket = buckets.name\n",
    "print(\"Bucket: \", bucket)\n",
    "\n",
    "prefix = 'scripts/data'\n",
    "S3Downloader.download(s3_uri=f\"s3://{bucket}/{prefix}/abalone_data.csv\", local_path= 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.2: Create a processing container\n",
    "\n",
    "Define and create a scikit-learn container by using the Dockerfile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2.1: Create a Dockerfile\n",
    "\n",
    "Create a Docker directory and add the Dockerfile that creates the processing container. Because you are creating a scikit-learn container, you install pandas and scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker/Dockerfile\n",
    "FROM public.ecr.aws/docker/library/python:3.10-slim-bullseye\n",
    "\n",
    "RUN pip3 install pandas scikit-learn\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2.2: Build the container image\n",
    "\n",
    "Create a custom container image by using the Amazon SageMaker Studio Image Build command line interface (CLI).\n",
    "\n",
    "By using the Amazon SageMaker Studio Image Build CLI, you can build Amazon SageMaker compatible Docker images directly from your SageMaker Studio environments. Using the Image Build CLI helps save time and increase security because it abstracts the creation of the build environment and requires fewer permissions.\n",
    "\n",
    "Navigate to the directory that contains your Dockerfile and run the sm-docker build command. This command automatically logs build output and returns the **Image URI** of your Docker image. This step takes 2–5 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "sudo rm /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n",
    "\n",
    "sudo cp /opt/conda/lib/libstdc++.so.6 /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n",
    "\n",
    "cd docker\n",
    "\n",
    "sm-docker build ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the cell completes, an Image URI is returned that looks like *012345678910.dkr.ecr.us-east-1.amazonaws.com/sagemaker-studio-d-vcbyjgmmjzzy:data-scientist-test-user*.\n",
    "\n",
    "1. Copy the **Image URI** and paste it into a text editor of your choice. \n",
    "\n",
    "You use this **Image URI** to create a **ScriptProcessor** class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.3: Run the SageMaker processing job\n",
    "\n",
    "AnyCompany Consulting is working on a project with a wildlife group that is studying abalone age. An abalone is a type of mollusk or marine snail. They want to predict the age of live specimens instead of having to cut open their shells to determine their age.\n",
    "\n",
    "The abalone dataset represents a population of over 4000 abalones. The dataset includes columns for sex, length, diameter, height, whole weight, shucked weight, viscera weight, shell weight, and rings.\n",
    "\n",
    "Run a processing job on the abalone dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import-data\n",
    "shape=pd.read_csv(\"data/abalone_data.csv\", header=0)\n",
    "shape.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, use the SageMaker ScriptProcessor class to define and run a processing script as a processing job. Refer to [SageMaker ScriptProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ScriptProcessor) for more information about this class.\n",
    "\n",
    "For creating the ScriptProcessor class, you configure the following parameters:\n",
    "- **base_job_name**: Prefix for the processing job name\n",
    "- **command**: Command to run, in addition to any command-line flags\n",
    "- **image_uri**: URI of the Docker image to use for the processing jobs\n",
    "- **role**: SageMaker execution role\n",
    "- **instance_count**: Number of instances to run the processing job\n",
    "- **instance_type**: Type of Amazon Elastic Compute Cloud (Amazon EC2) instance that is used for the processing job\n",
    "\n",
    "1. In the following code, replace **REPLACE_IMAGE_URI** with the URI from your text editor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sagemaker-script-processor\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "# create a ScriptProcessor\n",
    "script_processor = ScriptProcessor(\n",
    "    base_job_name=\"own-processing-container\",\n",
    "    command=[\"python3\"],\n",
    "    image_uri=\"REPLACE_IMAGE_URI\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the ScriptProcessor.run() method to run the **sklearn_preprocessing.py** script as a processing job. Refer to [ScriptProcessor.run()](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ScriptProcessor.run) for more information about this method.\n",
    "\n",
    "For running the processing job, you configure the following parameters:\n",
    "- **code**: Path of the preprocessing script \n",
    "- **inputs**: Path of input data for the preprocessing script (Amazon S3 input location)\n",
    "- **outputs**: Path of output for the preprocessing script (Amazon S3 output location)\n",
    "- **arguments**: Command-line arguments to the preprocessing script (such as train test split ratio)\n",
    "\n",
    "The processing job takes approximately 4–5 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing-job\n",
    "import os\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "# Amazon S3 path prefix\n",
    "input_raw_data_prefix = \"scripts/data\"\n",
    "output_preprocessed_data_prefix = \"scripts/data/output\"\n",
    "scripts_prefix = \"scripts/smstudiofiles\"\n",
    "logs_prefix = \"logs\"\n",
    "\n",
    "# Run the processing job\n",
    "script_processor.run(\n",
    "    code=\"s3://\" + os.path.join(bucket, scripts_prefix, \"sklearn_preprocessing.py\"),\n",
    "    inputs=[ProcessingInput(source=\"s3://\" + os.path.join(bucket, input_raw_data_prefix, \"abalone_data.csv\"),\n",
    "                            destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\", \n",
    "                        source=\"/opt/ml/processing/train\",\n",
    "                        destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"train\")),\n",
    "        ProcessingOutput(output_name=\"test_data\", \n",
    "                        source=\"/opt/ml/processing/test\",\n",
    "                        destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"test\")),\n",
    "    ],\n",
    "    arguments=[\"--train-test-split-ratio\", \"0.2\"],\n",
    ")\n",
    "script_processor_job_description = script_processor.jobs[-1].describe()\n",
    "print(script_processor_job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.4: Validate the data processing results\n",
    "\n",
    "Validate the output of the processing job that you ran by looking at the first five rows of the train and test output datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-train-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/train/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/train/train_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-validation-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/validation/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/test/test_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the cells complete, responses are returned that look like the following output:\n",
    "\n",
    "```plain\n",
    "Top 5 rows from s3://databucket-us-east-1-xxxxxxx/scripts/data/output/validation/\n",
    "M,0.55,0.425,0.155,0.918,0.278,0.243,0.335\n",
    "I,0.5,0.4,0.12,0.616,0.261,0.143,0.194\n",
    "M,0.62,0.48,0.155,1.256,0.527,0.374,0.318\n",
    "I,0.22,0.165,0.055,0.055,0.022,0.012,0.02\n",
    "M,0.645,0.5,0.175,1.511,0.674,0.376,0.378\n",
    "```\n",
    "\n",
    "The column headers for the abalone dataset are sex (Infant, Male, Female), length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight, and rings. The output from the **train/** and **validation/** folders shows the processed data that is stored in your S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have built your own processing container and used SageMaker Processing to run the processing job on that custom container.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with the **Conclusion**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
